{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "557f72ef",
   "metadata": {},
   "source": [
    "# Dynamics between mental states over time\n",
    "\n",
    "In this notebook, we illustrate how to use the Wishart process to model the dynamic covariance between five mental states over time. The dataset that we use in this notebook is available at https://osf.io/j4fg8/ and is described in the following paper:\n",
    "\n",
    "Kossakowski, J. J., Groot, P. C., Haslbeck, J. M., Borsboom, D., & Wichers, M. (2017). Data from ‘critical slowing down as a personalized early warning signal for depression’. Journal of Open Psychology Data, 5(1), 1-1.\n",
    "\n",
    "### Preprocessing\n",
    "The data comes from a single subject who has been diagnosed with Major Depressive Disorder and monitored his mental state over the course of 237 days by filling in a questionnaire of daily life experiences several times a day. The subject had been using venlafaxine for 8.5 years, and this dosage is reduced gradually. We based our pre-processing on the following paper:\n",
    "\n",
    "Wichers, M., Groot, P. C., Psychosystems, E. S. M., & EWS Group. (2016). Critical slowing down as a personalized early warning signal for depression. Psychotherapy and psychosomatics, 85(2), 114-116."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63bc59ee",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "SELECTED_DEVICE = None\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = f''\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "import jax\n",
    "import sys\n",
    "jax.config.update(\"jax_default_device\", jax.devices()[0])\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrnd\n",
    "import distrax as dx\n",
    "import jaxkern as jk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfb = tfp.bijectors\n",
    "module_path = os.path.abspath(os.path.join('../bayesianmodels/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from uicsmodels.gaussianprocesses.fullwp import FullLatentWishartModel\n",
    "from uicsmodels.gaussianprocesses.wputil import vec2tril, tril2vec, construct_wishart, construct_wishart_Lvec\n",
    "from uicsmodels.gaussianprocesses.likelihoods import AbstractLikelihood\n",
    "from uicsmodels.gaussianprocesses.gputil import sample_predictive\n",
    "import time\n",
    "module_path = os.path.abspath(os.path.join('../BANNER/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import gpflow\n",
    "from gpflow.kernels import SharedIndependent, SquaredExponential, Matern12\n",
    "from gpflow import Parameter\n",
    "from gpflow.inducing_variables import InducingPoints, SharedIndependentInducingVariables\n",
    "import tensorflow as tf\n",
    "from src.likelihoods.WishartProcessLikelihood import WishartLikelihood\n",
    "from src.models.WishartProcess import WishartProcess\n",
    "from util.training_util import run_adam\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Load and select data:\n",
    "data = pd.read_csv('/home/heshui/bayesianmodels/simulations/ESMdata.csv')\n",
    "columns_affect = ['mood_irritat', 'mood_satisfi', 'mood_lonely', 'mood_anxious', 'mood_enthus', \n",
    "                  'mood_cheerf', 'mood_guilty', 'mood_doubt', 'mood_strong', 'pat_restl', 'pat_agitate']\n",
    "data_subset = data[['concentrat'] + columns_affect + ['pat_worry', 'mood_suspic']].dropna()\n",
    "x = np.array(data_subset['concentrat'])\n",
    "\n",
    "# A few variables have Likert scales from -3 to 3, so we make sure all Likert scales have the same range:\n",
    "other_likert_scale = ['mood_lonely', 'mood_anxious', 'mood_guilty']\n",
    "data_subset[other_likert_scale] += 4\n",
    "\n",
    "# Apply PCA with oblique rotation:\n",
    "fa = FactorAnalyzer(method='principal', rotation='promax')\n",
    "fa.fit(data_subset[columns_affect].to_numpy())\n",
    "Y_pca = fa.transform(data_subset[columns_affect].to_numpy())\n",
    "Y = np.concatenate((Y_pca, data_subset[['pat_worry', 'mood_suspic']].to_numpy()), axis=1) # Append suspicious and worrying.\n",
    "\n",
    "# Remove slow non-linear time trends:\n",
    "pf = PolynomialFeatures(degree=5)\n",
    "xp = pf.fit_transform(np.tile(x, (Y.shape[1], 1)).T)\n",
    "md2 = LinearRegression()\n",
    "md2.fit(xp, Y)\n",
    "trend = md2.predict(xp)\n",
    "Y = Y - trend\n",
    "\n",
    "# Scale day numbers to range [0, 1]:\n",
    "minx = np.min(x)\n",
    "maxx = np.max(x)\n",
    "x = (x - minx) / (maxx - minx)\n",
    "\n",
    "# We want to use only 25% of the data, to select a subset of the data: \n",
    "idx = np.arange(0, Y.shape[0], 4)\n",
    "Y = Y[idx, :]\n",
    "x = x[idx]\n",
    "x = np.reshape(x, (-1, 1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "58509b98",
   "metadata": {},
   "source": [
    "### Define likelihood with EMA mean\n",
    "We assume $Y_i \\sim \\mathcal{MVN}_d \\left( \\mu_i, \\Sigma_i \\right)$ with $\\mu_i$ being an exponential moving average function: $\\text{EMA}(y_{i+1, j}) = \\alpha [y_{ij} + (1 - \\alpha)y_{i-1, j} + (1 - \\alpha)^2 y_{i-2,j} + \\ldots + (1 - \\alpha)^{k-1}y_{i-(k-1),j}] $ and $k=10$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "909e7a80",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a mean function (this is mu_i for Y_i ~ MVN(mu_i, Sigma_i)):\n",
    "def ema(Y, k=10):\n",
    "    \"\"\" This function implements an exponential moving average mean function.  \n",
    "\n",
    "    Args:\n",
    "        Y: a matrix with observations of shape (number of observations, number of variables).\n",
    "        k: an integer representing the number of previous observations to take into account for computing the EMA (optional).\n",
    "\n",
    "    Returns:\n",
    "        A matrix of shape (number of observations, number of variables) with the moving average.\n",
    "    \"\"\"\n",
    "    alpha = 2 / (k + 1)\n",
    "    n = Y.shape[0]\n",
    "    exponents = jnp.power(1 - alpha, jnp.arange(k))\n",
    "    moving_average = jax.vmap(lambda y_d: alpha * jnp.convolve(y_d, exponents)[:n], in_axes=(1, ))(Y)\n",
    "    moving_average = moving_average.T\n",
    "    return moving_average\n",
    "\n",
    "\n",
    "class Wishart_with_EMA(AbstractLikelihood):\n",
    "\n",
    "    def __init__(self, nu, d, Y, k=10):\n",
    "        self.nu = nu\n",
    "        self.d = d\n",
    "        self.mean = ema(Y, k=k)\n",
    "\n",
    "    def link_function(self, f):\n",
    "        \"\"\"Identity function\n",
    "        \"\"\"\n",
    "        return f\n",
    "\n",
    "    def likelihood(self, params, f=None, Sigma=None, mean=None):\n",
    "        assert f is not None or Sigma is not None, 'Provide either f or Sigma'\n",
    "        if Sigma is None:\n",
    "            if jnp.ndim(f):\n",
    "                f = jnp.reshape(f, (-1, self.nu, self.d))\n",
    "            L_vec = params['L_vec']\n",
    "            L = vec2tril(L_vec, self.d)\n",
    "            Sigma = construct_wishart(F=f, L=L)\n",
    "        mean = self.mean if mean is None else mean\n",
    "        return dx.MultivariateNormalFullCovariance(loc=mean, covariance_matrix=Sigma)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6060fde0",
   "metadata": {},
   "source": [
    "### 10-fold cross validation with MCMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7d9a7f5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to save performance over 10 folds:\n",
    "ll_mcmc = np.zeros((10))\n",
    "\n",
    "# Model settings:\n",
    "gpkernel = jk.RBF() + jk.Matern12()\n",
    "num_burn = 5000000\n",
    "num_samples = 1000000 \n",
    "num_thin = 1000\n",
    "num_chains = 4\n",
    "num_thin_samples = num_samples // num_thin\n",
    "\n",
    "# 10 fold cross validation:\n",
    "n, d = Y.shape\n",
    "for fold in range(1, 11):\n",
    "    # Get training and testing data:\n",
    "    n_train = (n // 10) * fold\n",
    "    x_train, Y_train = x[:n_train], Y[:n_train]\n",
    "    x_test, Y_test = x[n_train:n_train+10], Y[n_train:n_train+10]\n",
    "    \n",
    "    # We combine the samples across multiple chains:\n",
    "    Sigma_pred = np.zeros((num_thin_samples, Y_test.shape[0], d, d)) # array to store samples across chains.\n",
    "    Sigmas_chains = np.zeros((num_thin_samples, num_chains, Y_test.shape[0] * len_l)) # used for convergence.\n",
    "    for repetition in range(num_chains):\n",
    "        # Set priors and initialize model:\n",
    "        len_l = int(d * (d+1) / 2)\n",
    "        priors = dict(kernel = [dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                                dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()))],\n",
    "                    likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "        model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "        model.likelihood = Wishart_with_EMA(d+1, d, Y_train)\n",
    "\n",
    "        # Inference with SMC:\n",
    "        start = time.time()\n",
    "        key = jrnd.PRNGKey(repetition)\n",
    "        states = model.inference(key, mode='gibbs', sampling_parameters=dict(num_burn=num_burn,\n",
    "                                                                            num_samples=num_samples,\n",
    "                                                                            num_thin=num_thin))\n",
    "        end = time.time()\n",
    "\n",
    "        # Compute posterior distribution:\n",
    "        key, subkey = jrnd.split(key, 2)\n",
    "        keys = jrnd.split(subkey, num_samples)\n",
    "        f_pred = jax.vmap(sample_predictive, \n",
    "                        in_axes=(0, None, None, 0, None, None, 0))(keys, \n",
    "                                                                    x_train[:, 0], x_test[:, 0],\n",
    "                                                                    model.states.position['f'], gpkernel, None, \n",
    "                                                                    [{'lengthscale': model.states.position['kernel'][0]['lengthscale'], 'variance': np.ones((1000,))},\n",
    "                                                                    {'lengthscale': model.states.position['kernel'][1]['lengthscale'], 'variance': np.ones((1000,))}])                                                                                                                                                      # {'lengthscale': result['lengthscale2'][0, :], 'variance': np.ones((1000,))}])\n",
    "        Sigma_chain = jax.vmap(construct_wishart_Lvec)(f_pred, model.states.position['likelihood']['L_vec'])\n",
    "        Sigmas_chains[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_thin_samples, -1))\n",
    "        indices = np.random.choice(np.arange(num_thin_samples), size=(num_thin_samples // num_chains,), replace=False)\n",
    "        Sigma_pred[repetition * (num_thin_samples // num_chains):(repetition+1) * (num_thin_samples // num_chains)] = Sigma_chain[indices, :, :, :]\n",
    "\n",
    "    # Compute convergence of covariance processes:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_chains))\n",
    "    print(rhat)\n",
    "    \n",
    "    # Get posterior predictive distribution (using the EMA function):\n",
    "    Y_pred = np.zeros((num_thin_samples, Y_test.shape[0], d))\n",
    "    mean_distribution = np.zeros((num_thin_samples, Y_test.shape[0], d))\n",
    "    for s in range(num_thin_samples): \n",
    "        Y_temp = Y_train\n",
    "        for i in range(Y_test.shape[0]): # we predict Y_test one step ahead, and then update the EMA.\n",
    "            mu = ema(Y_temp)\n",
    "            mean_distribution[s, i, :] = mu[-1, :]\n",
    "            Y_pred[s, i, :] = np.random.multivariate_normal(mu[-1, :], Sigma_pred[s, i])\n",
    "            Y_temp = np.concatenate((Y_temp, np.reshape(Y_pred[s, i, :], (1, -1))), axis=0)\n",
    "    ll_mcmc[fold-1] = np.sum(model.likelihood.likelihood(None, Sigma=np.mean(Sigma_pred, axis=0), mean=np.mean(mean_distribution, axis=0)).log_prob(Y_test))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c938d9b",
   "metadata": {},
   "source": [
    "### 10-fold cross validation with variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb3bf5ff",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define log_likelihood\n",
    "def log_likelihood(y_true, cov_est, mean_est=None):\n",
    "    cov_est = np.mean(cov_est, axis=0)\n",
    "    ll = np.zeros((y_true.shape[0]))\n",
    "    for n in range(y_true.shape[0]):\n",
    "        ll[n] = multivariate_normal.logpdf(y_true[n], mean_est[n], cov_est[n])\n",
    "    return np.sum(ll) / y_true.shape[0]\n",
    "\n",
    "\n",
    "# Array to save performance over 10 folds:\n",
    "ll_vi = np.zeros((10))\n",
    "lengthscale_rbf = [np.exp(np.random.normal()) for rep in range(4)]\n",
    "lengthscale_m12 = [np.exp(np.random.normal()) for rep in range(4)]\n",
    "\n",
    "# Model settings:\n",
    "num_iterations = 1000000 # until convergence of the ELBO.\n",
    "num_samples = 1000\n",
    "num_initializations = 4\n",
    "n, d = Y.shape\n",
    "nu = d + 1\n",
    "latent_dim = int(nu * d)\n",
    "\n",
    "# 10 fold cross validation:\n",
    "for fold in range(1, 11):\n",
    "    # Get training and testing data:\n",
    "    n_train = (n // 10) * fold\n",
    "    x_train, Y_train = x[:n_train], Y[:n_train]\n",
    "    x_train = np.tile(x_train, (1, d))\n",
    "    x_test, Y_test = x[n_train:n_train+10], Y[n_train:n_train+10]\n",
    "    x_test = np.tile(x_test, (1, d))\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    best_elbo = -np.inf\n",
    "    Sigmas_initializations = np.zeros((num_samples, num_initializations, n_train * len_l))\n",
    "    for repetition in range(num_initializations):\n",
    "        # Set GP kernel function, likelihood, and initialize model:\n",
    "        gpkernel = SharedIndependent(SquaredExponential(lengthscales=lengthscale_rbf[repetition], variance=1.0)\n",
    "                                     + Matern12(lengthscales=lengthscale_m12[repetition], variance=1.0), output_dim=latent_dim) \n",
    "        V = np.random.normal(size=int(d * nu // 2)) # initialize scale matrix.\n",
    "        V = Parameter(V)\n",
    "        inducing_points = tf.identity(x_train)\n",
    "        inducing_variable = SharedIndependentInducingVariables(InducingPoints(tf.identity(inducing_points)))\n",
    "        likelihood = WishartLikelihood(d, nu, A=V, N=n_train, R=3, additive_noise=True, model_inverse=False, model_mean=True) # mean function.\n",
    "        model = WishartProcess(gpkernel, likelihood, D=d, nu=nu, inducing_variable=inducing_variable, num_data=n_train)\n",
    "        gpflow.set_trainable(model.inducing_variable, False) # we do not want to use inducing variables.\n",
    "\n",
    "        # Inference until convergence of the ELBO:\n",
    "        start = time.time()\n",
    "        logf = run_adam(model, (x_train, Y_train), num_iterations, minibatch_size=n_train, learning_rate=0.001)\n",
    "        end = time.time()\n",
    "\n",
    "        # Store estimates with the best ELBO:\n",
    "        Sigma_chain = np.array(model.predict_mc(x_train, num_samples)) \n",
    "        if logf[-1] > best_elbo:\n",
    "            best_elbo = logf[-1]\n",
    "            Sigma_train = Sigma_chain\n",
    "            Sigmas_initializations[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_samples, -1))\n",
    "            Sigma_pred = np.array(model.predict_mc(x_test, num_samples)) \n",
    "\n",
    "    # Get posterior predictive distribution (using the EMA function):\n",
    "    Y_pred = np.zeros((num_samples, Y_test.shape[0], d))\n",
    "    mean_distribution = np.zeros((num_samples, Y_test.shape[0], d))\n",
    "    for s in range(num_samples): \n",
    "        Y_temp = Y_train\n",
    "        for i in range(Y_test.shape[0]): # we predict Y_test one step ahead, and then update the EMA.\n",
    "            mu = ema(Y_temp)\n",
    "            mean_distribution[s, i, :] = mu[-1, :]\n",
    "            Y_pred[s, i, :] = np.random.multivariate_normal(mu[-1, :], Sigma_pred[s, i])\n",
    "            Y_temp = np.concatenate((Y_temp, np.reshape(Y_pred[s, i, :], (1, -1))), axis=0)\n",
    "\n",
    "    # Compute convergence of covariance processes and log-likelihood:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_initializations))\n",
    "    print(rhat)\n",
    "    ll_vi[fold-1] = log_likelihood(Y_test, Sigma_pred, np.mean(mean_distribution, axis=0))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9603c870",
   "metadata": {},
   "source": [
    "### 10-fold cross validation with SMC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe98b2f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Array to save performance over 10 folds:\n",
    "ll_smc = np.zeros((10))\n",
    "\n",
    "# Model settings:\n",
    "gpkernel = jk.RBF() + jk.Matern12()\n",
    "num_particles = 1000\n",
    "num_mcmc_steps = 5000\n",
    "\n",
    "# 10 fold cross validation:\n",
    "n, d = Y.shape\n",
    "for fold in range(1, 11):\n",
    "    # Get training and testing data:\n",
    "    n_train = (n // 10) * fold\n",
    "    x_train, Y_train = x[:n_train], Y[:n_train]\n",
    "    x_test, Y_test = x[n_train:n_train+10], Y[n_train:n_train+10]\n",
    "    \n",
    "    # Set priors and initialize model:\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    priors = dict(kernel = [dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                            dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()))],\n",
    "                likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "    model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "    model.likelihood = Wishart_with_EMA(d+1, d, Y_train)\n",
    "\n",
    "    # Inference with SMC:\n",
    "    start = time.time()\n",
    "    key = jrnd.PRNGKey(10)\n",
    "    particles, num_iter, lml = model.inference(key, mode='gibbs-in-smc', \n",
    "                                                sampling_parameters=dict(num_particles=num_particles, \n",
    "                                                                        num_mcmc_steps=num_mcmc_steps))\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute posterior distribution and the posterior predictive distribution (using the EMA function):\n",
    "    key = jrnd.PRNGKey(5)\n",
    "    Sigma_pred = model.predict_Sigma(key, x_test)\n",
    "    Y_pred = np.zeros((num_particles, Y_test.shape[0], d))\n",
    "    mean_distribution = np.zeros((num_particles, Y_test.shape[0], d))\n",
    "    for s in range(num_particles): \n",
    "        Y_temp = Y_train\n",
    "        for i in range(Y_test.shape[0]): # we predict Y_test one step ahead, and then update the EMA.\n",
    "            mu = ema(Y_temp)\n",
    "            mean_distribution[s, i, :] = mu[-1, :]\n",
    "            Y_pred[s, i, :] = np.random.multivariate_normal(mu[-1, :], Sigma_pred[s, i])\n",
    "            Y_temp = np.concatenate((Y_temp, np.reshape(Y_pred[s, i, :], (1, -1))), axis=0)\n",
    "    ll_smc[fold-1] = np.sum(model.likelihood.likelihood(None, Sigma=np.mean(Sigma_pred, axis=0), mean=np.mean(mean_distribution, axis=0)).log_prob(Y_test))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
