{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f8a911",
   "metadata": {},
   "source": [
    "# Simulation study: state switching and out-of-sample prediction using Wishart processes\n",
    "\n",
    "In this notebook, we illustrate the second simulation study of our work (https://arxiv.org/abs/2406.04796). In this simulation study, we construct an covariance process between $d=3$ variables from the prior of a Wishart process, and sample $n=600$ observations from a multivariate normal distribution with a mean of zero and the constructed covariance process, of which the first 300 observations are used for training, and the remaining 300 data points for out-of-sample prediction. We generate 10 different datasets with the same underlying covariance process, and compare MCMC, variational inference and SMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42663b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "SELECTED_DEVICE = None\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = f''\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "import jax\n",
    "import sys\n",
    "jax.config.update(\"jax_default_device\", jax.devices()[0])\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrnd\n",
    "import distrax as dx\n",
    "import jaxkern as jk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfb = tfp.bijectors\n",
    "module_path = os.path.abspath(os.path.join('../bayesianmodels/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from uicsmodels.gaussianprocesses.fullwp import FullLatentWishartModel\n",
    "from uicsmodels.gaussianprocesses.wputil import vec2tril, tril2vec, construct_wishart, construct_wishart_Lvec\n",
    "from uicsmodels.gaussianprocesses.likelihoods import AbstractLikelihood\n",
    "import time\n",
    "module_path = os.path.abspath(os.path.join('../BANNER/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import gpflow\n",
    "from gpflow.kernels import SharedIndependent, SquaredExponential, Periodic\n",
    "from gpflow import Parameter\n",
    "from gpflow.inducing_variables import InducingPoints, SharedIndependentInducingVariables\n",
    "import tensorflow as tf\n",
    "from src.likelihoods.WishartProcessLikelihood import WishartLikelihood\n",
    "from src.models.WishartProcess import WishartProcess\n",
    "from util.training_util import run_adam\n",
    "\n",
    "# Data settings:\n",
    "n = 600\n",
    "d = 3\n",
    "\n",
    "# Generate data:\n",
    "x = np.reshape(np.linspace(0, 2, n), (-1, 1))\n",
    "Y = np.zeros((n, 10, d))\n",
    "true_Sigma = np.zeros((n, d, d))\n",
    "r = 0.8\n",
    "for trial in range(10):\n",
    "    for i in range(n):\n",
    "        if i % 50 == 0:\n",
    "            if r == 0.8:\n",
    "                r = 0.0\n",
    "            else:\n",
    "                r = 0.8\n",
    "        true_Sigma[i] = np.array([[1., r, r], [r, 1, r], [r, r, 1.]])\n",
    "        Y[i, trial] = np.random.multivariate_normal(mean=np.zeros((d,)), cov=true_Sigma[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe2f009",
   "metadata": {},
   "source": [
    "### MCMC sampling\n",
    "We start by modelling the covariance process using a Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17934231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.Periodic()\n",
    "num_burn = 5000000\n",
    "num_samples = 1000000\n",
    "num_thin = 1000\n",
    "num_chains = 4\n",
    "num_thin_samples = num_samples // num_thin\n",
    "_, num_trials, d = Y.shape\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    # We combine the samples across multiple chains:\n",
    "    Sigma_train = np.zeros((num_thin_samples, n_train, d, d)) # array to store samples across chains.\n",
    "    Sigma_pred = np.zeros((num_thin_samples, n_test, d, d))\n",
    "    Sigmas_chains = np.zeros((num_thin_samples, num_chains, n_train * len_l))\n",
    "    for repetition in range(num_chains):\n",
    "        # Set priors and initialize model:\n",
    "        priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                    period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                      likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "        model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "\n",
    "        # Inference with SMC:\n",
    "        start = time.time()\n",
    "        key = jrnd.PRNGKey(repetition)\n",
    "        states = model.inference(key, mode='gibbs', sampling_parameters=dict(num_burn=num_burn,\n",
    "                                                                            num_samples=num_samples,\n",
    "                                                                            num_thin=num_thin))\n",
    "        end = time.time()\n",
    "\n",
    "        # Compute posterior distribution:\n",
    "        Sigma_chain = jax.vmap(construct_wishart_Lvec)(model.states.position['f'], model.states.position['likelihood']['L_vec'])\n",
    "        Sigmas_chains[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_thin_samples, -1))\n",
    "        indices = np.random.choice(np.arange(num_thin_samples), size=(num_thin_samples // num_chains,), replace=False)\n",
    "        Sigma_train[repetition * (num_thin_samples // num_chains):(repetition+1) * (num_thin_samples // num_chains)] = Sigma_chain[indices, :, :, :]\n",
    "        Sigma_pred = model.predict_Sigma(key, x_test)\n",
    "        \n",
    "    # Compute convergence of covariance processes:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_chains))\n",
    "    print(rhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e11f4a",
   "metadata": {},
   "source": [
    "Next, we use a Locally Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07384cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.RBF() * jk.Periodic()\n",
    "num_burn = 5000000\n",
    "num_samples = 1000000 \n",
    "num_thin = 1000 \n",
    "num_chains = 4\n",
    "num_thin_samples = num_samples // num_thin\n",
    "\n",
    "_, num_trials, d = Y.shape\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    # We combine the samples across multiple chains:\n",
    "    Sigma_train = np.zeros((num_thin_samples, n_train, d, d)) # array to store samples across chains.\n",
    "    Sigma_pred = np.zeros((num_thin_samples, n_test, d, d))\n",
    "    Sigmas_chains = np.zeros((num_thin_samples, num_chains, n_train * len_l))\n",
    "    for repetition in range(num_chains):\n",
    "        # Set priors and initialize model:\n",
    "        priors = dict(kernel = [dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                                dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                     period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()))],\n",
    "                      likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "        model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "\n",
    "        # Inference with SMC:\n",
    "        start = time.time()\n",
    "        key = jrnd.PRNGKey(repetition)\n",
    "        states = model.inference(key, mode='gibbs', sampling_parameters=dict(num_burn=num_burn,\n",
    "                                                                            num_samples=num_samples,\n",
    "                                                                            num_thin=num_thin))\n",
    "        end = time.time()\n",
    "\n",
    "        # Compute posterior distribution:\n",
    "        Sigma_chain = jax.vmap(construct_wishart_Lvec)(model.states.position['f'], model.states.position['likelihood']['L_vec'])\n",
    "        Sigmas_chains[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_thin_samples, -1))\n",
    "        indices = np.random.choice(np.arange(num_thin_samples), size=(num_thin_samples // num_chains,), replace=False)\n",
    "        Sigma_train[repetition * (num_thin_samples // num_chains):(repetition+1) * (num_thin_samples // num_chains)] = Sigma_chain[indices, :, :, :]\n",
    "        Sigma_pred = model.predict_Sigma(key, x_test)\n",
    "        \n",
    "    # Compute convergence of covariance processes:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_chains))\n",
    "    print(rhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993532e3",
   "metadata": {},
   "source": [
    "### Variational inference\n",
    "We again first try to model the covariance process using a Periodic covariance function, and then a Locally Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac573b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "num_iterations = 1000000 # until convergence of the ELBO.\n",
    "num_samples = 1000\n",
    "num_initializations = 4\n",
    "_, num_trials, d = Y.shape\n",
    "nu = d + 1\n",
    "latent_dim = int(nu * d)\n",
    "lengthscale = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "period = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    best_elbo = -np.inf \n",
    "    Sigmas_initializations = np.zeros((num_samples, num_initializations, n_train * len_l))\n",
    "    for repetition in range(num_initializations):\n",
    "        # Set GP kernel function, likelihood, and initialize model:\n",
    "        kernel = SharedIndependent(Periodic(SquaredExponential(lengthscales=lengthscale[repetition], variance=1.0), period=period[repetition]), output_dim=latent_dim)\n",
    "        V = np.random.normal(size=int(d * nu // 2)) # initialize scale matrix.\n",
    "        V = Parameter(V)\n",
    "        inducing_points = tf.identity(x_train)\n",
    "        inducing_variable = SharedIndependentInducingVariables(InducingPoints(tf.identity(inducing_points)))\n",
    "        likelihood = WishartLikelihood(d, nu, A=V, N=n_train, R=3, additive_noise=True, model_inverse=False)\n",
    "        model = WishartProcess(kernel, likelihood, D=d, nu=nu, inducing_variable=inducing_variable, num_data=n_train)\n",
    "        gpflow.set_trainable(model.inducing_variable, False) # we do not want to use inducing variables.\n",
    "\n",
    "        # Inference until convergence of the ELBO:\n",
    "        start = time.time()\n",
    "        logf = run_adam(model, (x_train, Y_train), num_iterations, minibatch_size=n_train, learning_rate=0.001)\n",
    "        end = time.time()\n",
    "\n",
    "        # Store estimates with the best ELBO:\n",
    "        Sigma_chain = np.array(model.predict_mc(x_train, num_samples)) \n",
    "        if logf[-1] > best_elbo:\n",
    "            best_elbo = logf[-1]\n",
    "            Sigma_train = Sigma_chain\n",
    "            Sigmas_initializations[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_samples, -1))\n",
    "            Sigma_pred = np.array(model.predict_mc(x_test, num_samples)) \n",
    "\n",
    "    # Compute convergence of covariance processes:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_initializations))\n",
    "    print(rhat)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "num_iterations = 1000000 # until convergence of the ELBO.\n",
    "num_samples = 1000\n",
    "num_initializations = 4\n",
    "_, num_trials, d = Y.shape\n",
    "nu = d + 1\n",
    "latent_dim = int(nu * d)\n",
    "lengthscale_rbf = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "lengthscale_periodic = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "period = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    best_elbo = -np.inf \n",
    "    Sigmas_initializations = np.zeros((num_samples, num_initializations, n_train * len_l))\n",
    "    for repetition in range(num_initializations):\n",
    "        # Set GP kernel function, likelihood, and initialize model:\n",
    "        kernel = SharedIndependent(SquaredExponential(lengthscales=lengthscale_rbf[repetition], variance=1.0) *\n",
    "                                   Periodic(SquaredExponential(lengthscales=lengthscale_periodic[repetition], variance=1.0), period=period[repetition]), output_dim=latent_dim)\n",
    "        V = np.random.normal(size=int(d * nu // 2)) # initialize scale matrix.\n",
    "        V = Parameter(V)\n",
    "        inducing_points = tf.identity(x_train)\n",
    "        inducing_variable = SharedIndependentInducingVariables(InducingPoints(tf.identity(inducing_points)))\n",
    "        likelihood = WishartLikelihood(d, nu, A=V, N=n_train, R=3, additive_noise=True, model_inverse=False)\n",
    "        model = WishartProcess(kernel, likelihood, D=d, nu=nu, inducing_variable=inducing_variable, num_data=n_train)\n",
    "        gpflow.set_trainable(model.inducing_variable, False) # we do not want to use inducing variables.\n",
    "\n",
    "        # Inference until convergence of the ELBO:\n",
    "        start = time.time()\n",
    "        logf = run_adam(model, (x_train, Y_train), num_iterations, minibatch_size=n_train, learning_rate=0.001)\n",
    "        end = time.time()\n",
    "\n",
    "        # Store estimates with the best ELBO:\n",
    "        Sigma_chain = np.array(model.predict_mc(x_train, num_samples)) \n",
    "        if logf[-1] > best_elbo:\n",
    "            best_elbo = logf[-1]\n",
    "            Sigma_train = Sigma_chain\n",
    "            Sigmas_initializations[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_samples, -1))\n",
    "            Sigma_pred = np.array(model.predict_mc(x_test, num_samples)) \n",
    "\n",
    "    # Compute convergence of covariance processes:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_initializations))\n",
    "    print(rhat)\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efd534",
   "metadata": {},
   "source": [
    "### SMC sampling\n",
    "First we will use a Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model settings:\n",
    "gpkernel = jk.Periodic()\n",
    "num_particles = 1000\n",
    "num_mcmc_steps = 3000\n",
    "_, num_trials, d = Y.shape\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "\n",
    "    # Set priors and initialize model:\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                  likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "    model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "    \n",
    "    # Inference with SMC:\n",
    "    start = time.time()\n",
    "    key = jrnd.PRNGKey(10)\n",
    "    particles, num_iter, lml = model.inference(key, mode='gibbs-in-smc', \n",
    "                                                sampling_parameters=dict(num_particles=num_particles, \n",
    "                                                                        num_mcmc_steps=num_mcmc_steps))\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute posterior distribution and make out-of-sample predictions:\n",
    "    key = jrnd.PRNGKey(5)\n",
    "    Sigma_train = jax.vmap(construct_wishart_Lvec)(particles.particles['f'], particles.particles['likelihood']['L_vec'])\n",
    "    Sigma_pred = model.predict_Sigma(key, x_test)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135beb0",
   "metadata": {},
   "source": [
    "Next, we multiply a Periodic and RBF covariance function to obtain a Locally Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model settings:\n",
    "gpkernel = jk.RBF() * jk.Periodic()\n",
    "num_particles = 1000\n",
    "num_mcmc_steps = 3000\n",
    "_, num_trials, d = Y.shape\n",
    "\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "\n",
    "    # Set priors and initialize model:\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    priors = dict(kernel = [dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                            dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                 period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()))],\n",
    "                likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "    model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "    \n",
    "    # Inference with SMC:\n",
    "    start = time.time()\n",
    "    key = jrnd.PRNGKey(10)\n",
    "    particles, num_iter, lml = model.inference(key, mode='gibbs-in-smc', \n",
    "                                                sampling_parameters=dict(num_particles=num_particles, \n",
    "                                                                        num_mcmc_steps=num_mcmc_steps))\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute posterior distribution:\n",
    "    key = jrnd.PRNGKey(5)\n",
    "    Sigma_train = jax.vmap(construct_wishart_Lvec)(particles.particles['f'], particles.particles['likelihood']['L_vec'])\n",
    "    Sigma_pred = model.predict_Sigma(key, x_test)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
