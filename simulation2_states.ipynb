{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "26f8a911",
   "metadata": {},
   "source": [
    "# Simulation study: state switching and out-of-sample prediction using Wishart processes\n",
    "\n",
    "In this notebook, we illustrate the second simulation study of our work (https://arxiv.org/abs/2406.04796). In this simulation study, we construct an covariance process between $d=3$ variables from the prior of a Wishart process, and sample $n=600$ observations from a multivariate normal distribution with a mean of zero and the constructed covariance process, of which the first 300 observations are used for training, and the remaining 300 data points for out-of-sample prediction. We generate 10 different datasets with the same underlying covariance process, and compare MCMC, variational inference and SMC."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42663b66",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "SELECTED_DEVICE = None\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = f''\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "import jax\n",
    "import sys\n",
    "jax.config.update(\"jax_default_device\", jax.devices()[0])\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrnd\n",
    "import distrax as dx\n",
    "import jaxkern as jk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "tfb = tfp.bijectors\n",
    "module_path = os.path.abspath(os.path.join('../bayesianmodels/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from uicsmodels.gaussianprocesses.fullwp import FullLatentWishartModel\n",
    "from uicsmodels.gaussianprocesses.wputil import vec2tril, tril2vec, construct_wishart, construct_wishart_Lvec\n",
    "from uicsmodels.gaussianprocesses.likelihoods import AbstractLikelihood\n",
    "import time\n",
    "module_path = os.path.abspath(os.path.join('../BANNER/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import gpflow\n",
    "from gpflow.kernels import SharedIndependent, SquaredExponential, Periodic\n",
    "from gpflow import Parameter\n",
    "from gpflow.inducing_variables import InducingPoints, SharedIndependentInducingVariables\n",
    "import tensorflow as tf\n",
    "from src.likelihoods.WishartProcessLikelihood import WishartLikelihood\n",
    "from src.models.WishartProcess import WishartProcess\n",
    "from util.training_util import run_adam\n",
    "\n",
    "# Data settings:\n",
    "n = 600\n",
    "d = 3\n",
    "\n",
    "# Generate data:\n",
    "x = np.reshape(np.linspace(0, 2, n), (-1, 1))\n",
    "Y = np.zeros((n, 10, d))\n",
    "true_Sigma = np.zeros((n, d, d))\n",
    "r = 0.8\n",
    "for trial in range(10):\n",
    "    for i in range(n):\n",
    "        if i % 50 == 0:\n",
    "            if r == 0.8:\n",
    "                r = 0.0\n",
    "            else:\n",
    "                r = 0.8\n",
    "        true_Sigma[i] = np.array([[1., r, r], [r, 1, r], [r, r, 1.]])\n",
    "        Y[i, trial] = np.random.multivariate_normal(mean=np.zeros((d,)), cov=true_Sigma[i])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "45db5ac7",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "We compare MCMC, VI and SMC based on how well they recover the ground truth. First, we evaluate the covariance process estimates (on the first $n = 300$ observations) by the mean squared error (MSE) between the ground truth covariance process and the corresponding mean estimate of the covariance process (averaged over $d$ variables and $n$ observations). Additionally, to evaluate the full posterior $\\textit{distribution}$, we compute the MSE for all individual samples of the covariance process, referred to as $\\text{MSE}_\\text{samples}$. This $\\text{MSE}_\\text{samples}$ is again averaged over all $d$ variables and $n$ observations. Next, we make $n = 300$ out-of-sample predictions, and evaluate these predictions using the MSE, $\\text{MSE}_\\text{samples}$, log-likelihood, and KL-divergence between the predictive posterior distribution and true distribution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ddb076b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(ground_truth, estimate):\n",
    "    \"\"\" Function to compute the mean squared error between a ground truth and a corresponding estimate.\n",
    "\n",
    "    Args:\n",
    "        ground_truth: a matrix or vector with the true values of the data.\n",
    "        estimate: a matrix or vector with the estimate(s) of the ground truth by the model. If 'estimate' is of\n",
    "                  exactly the same shape as 'ground_truth', the MSE is averaged over all dimensions (variables \n",
    "                  and observations). Alternatively, if 'estimate' consists of individual samples, the MSE is\n",
    "                  computed for each sample individually.\n",
    "\n",
    "    Returns: a scalar with the MSE or MSE_samples.\n",
    "    \"\"\"\n",
    "    if ground_truth.shape == estimate.shape: # MSE\n",
    "        return np.mean(np.square(ground_truth - estimate))\n",
    "    elif len(ground_truth.shape) < len(estimate.shape): # MSE_samples\n",
    "        mses = jax.vmap(lambda gt, est: jnp.mean(jnp.square(gt - est)), in_axes=(None, 0))(ground_truth, estimate)\n",
    "        return np.mean(mses)\n",
    "    \n",
    "\n",
    "def log_likelihood(Y, mean_estimate, cov_estimate):\n",
    "    \"\"\" Function to compute the log likelihood of observing the true observations given the estimated mean and covariance\n",
    "        process. We assume that Y ~ MVN(mean_estimate, cov_estimate). If a distribution over the mean or covariance process\n",
    "        is given, we first compute the average estimates of these.\n",
    "\n",
    "    Args:\n",
    "        Y: an (n, d) matrix with n observations for d variables.\n",
    "        mean_estimate: an (n, d) or (num_samples, n, d) array with the mean estimates.\n",
    "        cov_estimate: an (n, d, d) or (num_samples, n, d, d) array with the covariance process estimates.\n",
    "\n",
    "    Returns: a scalar with the log likelihood averaged over the number of observations.\n",
    "    \"\"\"\n",
    "    if len(mean_estimate.shape) == 4:\n",
    "        mean_estimate = np.mean(mean_estimate, axis=0)\n",
    "    if len(cov_estimate.shape) == 4:\n",
    "        cov_estimate = np.mean(cov_estimate, axis=0)\n",
    "\n",
    "    n = mean_estimate.shape[0]\n",
    "    ll = jax.vmap(lambda y, mu, cov: jax.scipy.stats.multivariate_normal.logpdf(y, mu, cov))(Y, mean_estimate, cov_estimate)\n",
    "    return np.sum(ll) / n\n",
    "\n",
    "\n",
    "def kl_divergence(key, mu, cov, Y_estimate):\n",
    "    \"\"\" This function computes the KL-divergence between the predictive posterior distribution Y_estimate and samples from the\n",
    "        true distribution of Y ~ MVN(mu, cov).\n",
    "    \n",
    "    Args: key: a jrnd PRNGKey, the random number seed.\n",
    "          mu: an (d,) vector with the true latent mean of the data at location n.\n",
    "          cov: an (d, d) matrix with the true latent covariance process of the data\n",
    "          Y_estimate: an (num_samples, d) array with the predictive poster distribution samples.\n",
    "\n",
    "    Returns: a scalar representing the KL-divergence.\n",
    "    \"\"\"\n",
    "    true_samples = jrnd.multivariate_normal(key, mu, cov, shape=(1000,))\n",
    "\n",
    "    # Turn the samples of MVN(mu, cov) and the predictive posterior into a distribution: \n",
    "    true_dist = jax.scipy.stats.gaussian_kde(true_samples.T) \n",
    "    est_dist = jax.scipy.stats.gaussian_kde(Y_estimate.T)\n",
    "    key, subkey = jrnd.split(key)\n",
    "    samples_true = true_dist.resample(subkey, (300, )) # sample 300 data points from the true distribution.\n",
    "    return jnp.log(true_dist.pdf(samples_true) / est_dist.pdf(samples_true) + 1e-6).mean()\n",
    "\n",
    "\n",
    "def L_to_V(L_sample, d):\n",
    "    \"\"\" This function computes the scale matrix from its lower Cholesky decomposition, and takes the upper triangular \n",
    "    elements of this matrix.\n",
    "\n",
    "    Args:\n",
    "        L_sample: a vector with the lower Cholesky decomposition of a scale matrix.\n",
    "        d: a scalar representing the number of variables.\n",
    "    \n",
    "    Returns: a vector with the upper triangular elements of the scale matrix corresponding to L_sample.\n",
    "    \"\"\" \n",
    "    L = jnp.zeros((d, d))\n",
    "    L = L.at[jnp.tril_indices(d)].set(L_sample)\n",
    "    return jnp.dot(L, L.T)[jnp.triu_indices(d)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "abe2f009",
   "metadata": {},
   "source": [
    "### MCMC sampling\n",
    "We start by modelling the covariance process using a Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "17934231",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.Periodic()\n",
    "num_burn = 5000000\n",
    "num_samples = 1000000\n",
    "num_thin = 1000\n",
    "num_chains = 4\n",
    "num_thin_samples = num_samples // num_thin\n",
    "_, num_trials, d = Y.shape\n",
    "\n",
    "mse_cov_train, mse_samples_cov_train, mse_cov_pred, mse_samples_cov_pred, ll, kl, rhats = [], [], [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    # We combine the samples across multiple chains:\n",
    "    Sigma_train = np.zeros((num_thin_samples, n_train, d, d)) # array to store samples across chains.\n",
    "    Sigma_pred = np.zeros((num_thin_samples, n_test, d, d))\n",
    "    Sigmas_chains = np.zeros((num_thin_samples, num_chains, n_train * len_l))\n",
    "    for repetition in range(num_chains):\n",
    "        # Set priors and initialize model:\n",
    "        priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                    period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                      likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "        model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "\n",
    "        # Inference with SMC:\n",
    "        start = time.time()\n",
    "        key = jrnd.PRNGKey(repetition)\n",
    "        states = model.inference(key, mode='gibbs', sampling_parameters=dict(num_burn=num_burn,\n",
    "                                                                            num_samples=num_samples,\n",
    "                                                                            num_thin=num_thin))\n",
    "        end = time.time()\n",
    "\n",
    "        # Compute posterior distribution:\n",
    "        Sigma_chain = jax.vmap(construct_wishart_Lvec)(model.states.position['f'], model.states.position['likelihood']['L_vec'])\n",
    "        Sigmas_chains[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_thin_samples, -1))\n",
    "        indices = np.random.choice(np.arange(num_thin_samples), size=(num_thin_samples // num_chains,), replace=False)\n",
    "        Sigma_train[repetition * (num_thin_samples // num_chains):(repetition+1) * (num_thin_samples // num_chains)] = Sigma_chain[indices, :, :, :]\n",
    "        Sigma_pred = model.predict_Sigma(key, x_test)\n",
    "        \n",
    "    # Evaluate performance per trial:\n",
    "    true_Sigma_train_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[:300])).flatten()\n",
    "    Sigma_train_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_train), (num_thin_samples, -1))\n",
    "    mse_cov_train.append(mse(true_Sigma_train_triu, np.mean(Sigma_train_triu, axis=0)))\n",
    "    mse_samples_cov_train.append(mse(true_Sigma_train_triu, Sigma_train_triu))\n",
    "    true_Sigma_pred_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[300:])).flatten()\n",
    "    Sigma_pred_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_pred), (num_thin_samples, -1))\n",
    "    mse_cov_pred.append(mse(true_Sigma_pred_triu, np.mean(Sigma_pred_triu, axis=0)))\n",
    "    mse_samples_cov_pred.append(mse(true_Sigma_pred_triu, Sigma_pred_triu))\n",
    "    ll.append(log_likelihood(Y_test, np.zeros((n_test, d, d)), Sigma_pred))\n",
    "\n",
    "    # Obtain the predictive posterior distribution and compute the KL-divergence:\n",
    "    keys = jrnd.split(key, n_test)\n",
    "    Y_pred = jax.vmap(lambda Sigma_sample: jax.vmap(lambda key, mu, cov: jrnd.multivariate_normal(key, mu, cov))(keys, jnp.zeros((n_test, d)), Sigma_sample))(Sigma_pred)\n",
    "    keys = jrnd.split(keys[-1], n_test)\n",
    "    k = jax.vmap(kl_divergence, in_axes=(0, 0, 0, 1))(keys, np.zeros((n_test, d)), true_Sigma[300:], Y_pred)\n",
    "    kl.append(np.mean(k))\n",
    "\n",
    "    # Compute convergence of covariance processes:\n",
    "    rhats.append(np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_chains)))\n",
    "\n",
    "# Print performance:\n",
    "print('PSRF: ' + str(np.mean(rhats)) + u\"\\u00B1\" + str(np.std(rhats)))\n",
    "print('MSE (train): ' + str(np.mean(mse_cov_train)) + u\"\\u00B1\" + str(np.std(mse_cov_train)))\n",
    "print('MSE samples (train): ' + str(np.mean(mse_samples_cov_train)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_train)))\n",
    "print('MSE (test): ' + str(np.mean(mse_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_cov_pred)))\n",
    "print('MSE samples (test): ' + str(np.mean(mse_samples_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_pred)))\n",
    "print('LL (test): ' + str(np.mean(ll)) + u\"\\u00B1\" + str(np.std(ll)))\n",
    "print('KL (test): ' + str(np.mean(kl)) + u\"\\u00B1\" + str(np.std(kl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "71e11f4a",
   "metadata": {},
   "source": [
    "Next, we use a Locally Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07384cc4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.RBF() * jk.Periodic()\n",
    "num_burn = 5000000\n",
    "num_samples = 1000000 \n",
    "num_thin = 1000 \n",
    "num_chains = 4\n",
    "num_thin_samples = num_samples // num_thin\n",
    "\n",
    "_, num_trials, d = Y.shape\n",
    "mse_cov_train, mse_samples_cov_train, mse_cov_pred, mse_samples_cov_pred, ll, kl, rhats = [], [], [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    # We combine the samples across multiple chains:\n",
    "    Sigma_train = np.zeros((num_thin_samples, n_train, d, d)) # array to store samples across chains.\n",
    "    Sigma_pred = np.zeros((num_thin_samples, n_test, d, d))\n",
    "    Sigmas_chains = np.zeros((num_thin_samples, num_chains, n_train * len_l))\n",
    "    for repetition in range(num_chains):\n",
    "        # Set priors and initialize model:\n",
    "        priors = dict(kernel = [dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                                dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                     period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()))],\n",
    "                      likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "        model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "\n",
    "        # Inference with SMC:\n",
    "        start = time.time()\n",
    "        key = jrnd.PRNGKey(repetition)\n",
    "        states = model.inference(key, mode='gibbs', sampling_parameters=dict(num_burn=num_burn,\n",
    "                                                                            num_samples=num_samples,\n",
    "                                                                            num_thin=num_thin))\n",
    "        end = time.time()\n",
    "\n",
    "        # Compute posterior distribution:\n",
    "        Sigma_chain = jax.vmap(construct_wishart_Lvec)(model.states.position['f'], model.states.position['likelihood']['L_vec'])\n",
    "        Sigmas_chains[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_thin_samples, -1))\n",
    "        indices = np.random.choice(np.arange(num_thin_samples), size=(num_thin_samples // num_chains,), replace=False)\n",
    "        Sigma_train[repetition * (num_thin_samples // num_chains):(repetition+1) * (num_thin_samples // num_chains)] = Sigma_chain[indices, :, :, :]\n",
    "        Sigma_pred = model.predict_Sigma(key, x_test)\n",
    "    \n",
    "    # Evaluate performance per trial:\n",
    "    true_Sigma_train_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[:300])).flatten()\n",
    "    Sigma_train_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_train), (num_thin_samples, -1))\n",
    "    mse_cov_train.append(mse(true_Sigma_train_triu, np.mean(Sigma_train_triu, axis=0)))\n",
    "    mse_samples_cov_train.append(mse(true_Sigma_train_triu, Sigma_train_triu))\n",
    "    true_Sigma_pred_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[300:])).flatten()\n",
    "    Sigma_pred_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_pred), (num_thin_samples, -1))\n",
    "    mse_cov_pred.append(mse(true_Sigma_pred_triu, np.mean(Sigma_pred_triu, axis=0)))\n",
    "    mse_samples_cov_pred.append(mse(true_Sigma_pred_triu, Sigma_pred_triu))\n",
    "    ll.append(log_likelihood(Y_test, np.zeros((n_test, d, d)), Sigma_pred))\n",
    "\n",
    "    # Obtain the predictive posterior distribution and compute the KL-divergence:\n",
    "    keys = jrnd.split(key, n_test)\n",
    "    Y_pred = jax.vmap(lambda Sigma_sample: jax.vmap(lambda key, mu, cov: jrnd.multivariate_normal(key, mu, cov))(keys, jnp.zeros((n_test, d)), Sigma_sample))(Sigma_pred)\n",
    "    keys = jrnd.split(keys[-1], n_test)\n",
    "    k = jax.vmap(kl_divergence, in_axes=(0, 0, 0, 1))(keys, np.zeros((n_test, d)), true_Sigma[300:], Y_pred)\n",
    "    kl.append(np.mean(k))\n",
    "\n",
    "    # Compute convergence of covariance processes:\n",
    "    rhats.append(np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_chains)))\n",
    "\n",
    "# Print performance:\n",
    "print('PSRF: ' + str(np.mean(rhats)) + u\"\\u00B1\" + str(np.std(rhats)))\n",
    "print('MSE (train): ' + str(np.mean(mse_cov_train)) + u\"\\u00B1\" + str(np.std(mse_cov_train)))\n",
    "print('MSE samples (train): ' + str(np.mean(mse_samples_cov_train)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_train)))\n",
    "print('MSE (test): ' + str(np.mean(mse_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_cov_pred)))\n",
    "print('MSE samples (test): ' + str(np.mean(mse_samples_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_pred)))\n",
    "print('LL (test): ' + str(np.mean(ll)) + u\"\\u00B1\" + str(np.std(ll)))\n",
    "print('KL (test): ' + str(np.mean(kl)) + u\"\\u00B1\" + str(np.std(kl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "993532e3",
   "metadata": {},
   "source": [
    "### Variational inference\n",
    "We again first try to model the covariance process using a Periodic covariance function, and then a Locally Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6ac573b4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "num_iterations = 1000000 # until convergence of the ELBO.\n",
    "num_samples = 1000\n",
    "num_initializations = 4\n",
    "_, num_trials, d = Y.shape\n",
    "nu = d + 1\n",
    "latent_dim = int(nu * d)\n",
    "lengthscale = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "period = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "\n",
    "mse_cov_train, mse_samples_cov_train, mse_cov_pred, mse_samples_cov_pred, ll, kl, rhats = [], [], [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    best_elbo = -np.inf \n",
    "    Sigmas_initializations = np.zeros((num_samples, num_initializations, n_train * len_l))\n",
    "    for repetition in range(num_initializations):\n",
    "        # Set GP kernel function, likelihood, and initialize model:\n",
    "        kernel = SharedIndependent(Periodic(SquaredExponential(lengthscales=lengthscale[repetition], variance=1.0), period=period[repetition]), output_dim=latent_dim)\n",
    "        V = np.random.normal(size=int(d * nu // 2)) # initialize scale matrix.\n",
    "        V = Parameter(V)\n",
    "        inducing_points = tf.identity(x_train)\n",
    "        inducing_variable = SharedIndependentInducingVariables(InducingPoints(tf.identity(inducing_points)))\n",
    "        likelihood = WishartLikelihood(d, nu, A=V, N=n_train, R=3, additive_noise=True, model_inverse=False)\n",
    "        model = WishartProcess(kernel, likelihood, D=d, nu=nu, inducing_variable=inducing_variable, num_data=n_train)\n",
    "        gpflow.set_trainable(model.inducing_variable, False) # we do not want to use inducing variables.\n",
    "\n",
    "        # Inference until convergence of the ELBO:\n",
    "        start = time.time()\n",
    "        logf = run_adam(model, (x_train, Y_train), num_iterations, minibatch_size=n_train, learning_rate=0.001)\n",
    "        end = time.time()\n",
    "\n",
    "        # Store estimates with the best ELBO:\n",
    "        Sigma_chain = np.array(model.predict_mc(x_train, num_samples)) \n",
    "        if logf[-1] > best_elbo:\n",
    "            best_elbo = logf[-1]\n",
    "            Sigma_train = Sigma_chain\n",
    "            Sigmas_initializations[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_samples, -1))\n",
    "            Sigma_pred = np.array(model.predict_mc(x_test, num_samples)) \n",
    "\n",
    "    # Evaluate performance per trial:\n",
    "    true_Sigma_train_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[:300])).flatten()\n",
    "    Sigma_train_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_train), (num_samples, -1))\n",
    "    mse_cov_train.append(mse(true_Sigma_train_triu, np.mean(Sigma_train_triu, axis=0)))\n",
    "    mse_samples_cov_train.append(mse(true_Sigma_train_triu, Sigma_train_triu))\n",
    "    true_Sigma_pred_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[300:])).flatten()\n",
    "    Sigma_pred_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_pred), (num_samples, -1))\n",
    "    mse_cov_pred.append(mse(true_Sigma_pred_triu, np.mean(Sigma_pred_triu, axis=0)))\n",
    "    mse_samples_cov_pred.append(mse(true_Sigma_pred_triu, Sigma_pred_triu))\n",
    "    ll.append(log_likelihood(Y_test, np.zeros((n_test, d, d)), Sigma_pred))\n",
    "\n",
    "    # Obtain the predictive posterior distribution and compute the KL-divergence:\n",
    "    keys = jrnd.split(key, n_test)\n",
    "    Y_pred = jax.vmap(lambda Sigma_sample: jax.vmap(lambda key, mu, cov: jrnd.multivariate_normal(key, mu, cov))(keys, jnp.zeros((n_test, d)), Sigma_sample))(Sigma_pred)\n",
    "    keys = jrnd.split(keys[-1], n_test)\n",
    "    k = jax.vmap(kl_divergence, in_axes=(0, 0, 0, 1))(keys, np.zeros((n_test, d)), true_Sigma[300:], Y_pred)\n",
    "    kl.append(np.mean(k))\n",
    "            \n",
    "    # Compute convergence of covariance processes:\n",
    "    rhats.append(np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_initializations)))\n",
    "\n",
    "# Print performance:\n",
    "print('PSRF: ' + str(np.mean(rhats)) + u\"\\u00B1\" + str(np.std(rhats)))\n",
    "print('MSE (train): ' + str(np.mean(mse_cov_train)) + u\"\\u00B1\" + str(np.std(mse_cov_train)))\n",
    "print('MSE samples (train): ' + str(np.mean(mse_samples_cov_train)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_train)))\n",
    "print('MSE (test): ' + str(np.mean(mse_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_cov_pred)))\n",
    "print('MSE samples (test): ' + str(np.mean(mse_samples_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_pred)))\n",
    "print('LL (test): ' + str(np.mean(ll)) + u\"\\u00B1\" + str(np.std(ll)))\n",
    "print('KL (test): ' + str(np.mean(kl)) + u\"\\u00B1\" + str(np.std(kl)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "994fa22e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "num_iterations = 1000000 # until convergence of the ELBO.\n",
    "num_samples = 1000\n",
    "num_initializations = 4\n",
    "_, num_trials, d = Y.shape\n",
    "nu = d + 1\n",
    "latent_dim = int(nu * d)\n",
    "lengthscale_rbf = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "lengthscale_periodic = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "period = [np.exp(np.random.normal()) for rep in range(num_initializations)]\n",
    "\n",
    "mse_cov_train, mse_samples_cov_train, mse_cov_pred, mse_samples_cov_pred, ll, kl, rhats = [], [], [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    best_elbo = -np.inf \n",
    "    Sigmas_initializations = np.zeros((num_samples, num_initializations, n_train * len_l))\n",
    "    for repetition in range(num_initializations):\n",
    "        # Set GP kernel function, likelihood, and initialize model:\n",
    "        kernel = SharedIndependent(SquaredExponential(lengthscales=lengthscale_rbf[repetition], variance=1.0) *\n",
    "                                   Periodic(SquaredExponential(lengthscales=lengthscale_periodic[repetition], variance=1.0), period=period[repetition]), output_dim=latent_dim)\n",
    "        V = np.random.normal(size=int(d * nu // 2)) # initialize scale matrix.\n",
    "        V = Parameter(V)\n",
    "        inducing_points = tf.identity(x_train)\n",
    "        inducing_variable = SharedIndependentInducingVariables(InducingPoints(tf.identity(inducing_points)))\n",
    "        likelihood = WishartLikelihood(d, nu, A=V, N=n_train, R=3, additive_noise=True, model_inverse=False)\n",
    "        model = WishartProcess(kernel, likelihood, D=d, nu=nu, inducing_variable=inducing_variable, num_data=n_train)\n",
    "        gpflow.set_trainable(model.inducing_variable, False) # we do not want to use inducing variables.\n",
    "\n",
    "        # Inference until convergence of the ELBO:\n",
    "        start = time.time()\n",
    "        logf = run_adam(model, (x_train, Y_train), num_iterations, minibatch_size=n_train, learning_rate=0.001)\n",
    "        end = time.time()\n",
    "\n",
    "        # Store estimates with the best ELBO:\n",
    "        Sigma_chain = np.array(model.predict_mc(x_train, num_samples)) \n",
    "        if logf[-1] > best_elbo:\n",
    "            best_elbo = logf[-1]\n",
    "            Sigma_train = Sigma_chain\n",
    "            Sigmas_initializations[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_samples, -1))\n",
    "            Sigma_pred = np.array(model.predict_mc(x_test, num_samples)) \n",
    "\n",
    "    # Evaluate performance per trial:\n",
    "    true_Sigma_train_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[:300])).flatten()\n",
    "    Sigma_train_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_train), (num_samples, -1))\n",
    "    mse_cov_train.append(mse(true_Sigma_train_triu, np.mean(Sigma_train_triu, axis=0)))\n",
    "    mse_samples_cov_train.append(mse(true_Sigma_train_triu, Sigma_train_triu))\n",
    "    true_Sigma_pred_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[300:])).flatten()\n",
    "    Sigma_pred_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_pred), (num_samples, -1))\n",
    "    mse_cov_pred.append(mse(true_Sigma_pred_triu, np.mean(Sigma_pred_triu, axis=0)))\n",
    "    mse_samples_cov_pred.append(mse(true_Sigma_pred_triu, Sigma_pred_triu))\n",
    "    ll.append(log_likelihood(Y_test, np.zeros((n_test, d, d)), Sigma_pred))\n",
    "\n",
    "    # Obtain the predictive posterior distribution and compute the KL-divergence:\n",
    "    keys = jrnd.split(key, n_test)\n",
    "    Y_pred = jax.vmap(lambda Sigma_sample: jax.vmap(lambda key, mu, cov: jrnd.multivariate_normal(key, mu, cov))(keys, jnp.zeros((n_test, d)), Sigma_sample))(Sigma_pred)\n",
    "    keys = jrnd.split(keys[-1], n_test)\n",
    "    k = jax.vmap(kl_divergence, in_axes=(0, 0, 0, 1))(keys, np.zeros((n_test, d)), true_Sigma[300:], Y_pred)\n",
    "    kl.append(np.mean(k))\n",
    "            \n",
    "    # Compute convergence of covariance processes:\n",
    "    rhats.append(np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_initializations)))\n",
    "\n",
    "# Print performance:\n",
    "print('PSRF: ' + str(np.mean(rhats)) + u\"\\u00B1\" + str(np.std(rhats)))\n",
    "print('MSE (train): ' + str(np.mean(mse_cov_train)) + u\"\\u00B1\" + str(np.std(mse_cov_train)))\n",
    "print('MSE samples (train): ' + str(np.mean(mse_samples_cov_train)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_train)))\n",
    "print('MSE (test): ' + str(np.mean(mse_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_cov_pred)))\n",
    "print('MSE samples (test): ' + str(np.mean(mse_samples_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_pred)))\n",
    "print('LL (test): ' + str(np.mean(ll)) + u\"\\u00B1\" + str(np.std(ll)))\n",
    "print('KL (test): ' + str(np.mean(kl)) + u\"\\u00B1\" + str(np.std(kl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47efd534",
   "metadata": {},
   "source": [
    "### SMC sampling\n",
    "First we will use a Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69c3a7b1",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model settings:\n",
    "gpkernel = jk.Periodic()\n",
    "num_particles = 1000\n",
    "num_mcmc_steps = 3000\n",
    "_, num_trials, d = Y.shape\n",
    "\n",
    "mse_cov_train, mse_samples_cov_train, mse_cov_pred, mse_samples_cov_pred, ll, kl = [], [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "\n",
    "    # Set priors and initialize model:\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                  likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "    model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "    \n",
    "    # Inference with SMC:\n",
    "    start = time.time()\n",
    "    key = jrnd.PRNGKey(10)\n",
    "    particles, num_iter, lml = model.inference(key, mode='gibbs-in-smc', \n",
    "                                                sampling_parameters=dict(num_particles=num_particles, \n",
    "                                                                        num_mcmc_steps=num_mcmc_steps))\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute posterior distribution and make out-of-sample predictions:\n",
    "    key = jrnd.PRNGKey(5)\n",
    "    Sigma_train = jax.vmap(construct_wishart_Lvec)(particles.particles['f'], particles.particles['likelihood']['L_vec'])\n",
    "    Sigma_pred = model.predict_Sigma(key, x_test)\n",
    "\n",
    "    # Evaluate performance per trial:\n",
    "    true_Sigma_train_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[:300])).flatten()\n",
    "    Sigma_train_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_train), (num_particles, -1))\n",
    "    mse_cov_train.append(mse(true_Sigma_train_triu, np.mean(Sigma_train_triu, axis=0)))\n",
    "    mse_samples_cov_train.append(mse(true_Sigma_train_triu, Sigma_train_triu))\n",
    "    true_Sigma_pred_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[300:])).flatten()\n",
    "    Sigma_pred_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_pred), (num_particles, -1))\n",
    "    mse_cov_pred.append(mse(true_Sigma_pred_triu, np.mean(Sigma_pred_triu, axis=0)))\n",
    "    mse_samples_cov_pred.append(mse(true_Sigma_pred_triu, Sigma_pred_triu))\n",
    "    ll.append(log_likelihood(Y_test, np.zeros((n_test, d, d)), Sigma_pred))\n",
    "\n",
    "    # Obtain the predictive posterior distribution and compute the KL-divergence:\n",
    "    keys = jrnd.split(key, n_test)\n",
    "    Y_pred = jax.vmap(lambda Sigma_sample: jax.vmap(lambda key, mu, cov: jrnd.multivariate_normal(key, mu, cov))(keys, jnp.zeros((n_test, d)), Sigma_sample))(Sigma_pred)\n",
    "    keys = jrnd.split(keys[-1], n_test)\n",
    "    k = jax.vmap(kl_divergence, in_axes=(0, 0, 0, 1))(keys, np.zeros((n_test, d)), true_Sigma[300:], Y_pred)\n",
    "    kl.append(np.mean(k))\n",
    "\n",
    "# Print performance:\n",
    "print('MSE (train): ' + str(np.mean(mse_cov_train)) + u\"\\u00B1\" + str(np.std(mse_cov_train)))\n",
    "print('MSE samples (train): ' + str(np.mean(mse_samples_cov_train)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_train)))\n",
    "print('MSE (test): ' + str(np.mean(mse_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_cov_pred)))\n",
    "print('MSE samples (test): ' + str(np.mean(mse_samples_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_pred)))\n",
    "print('LL (test): ' + str(np.mean(ll)) + u\"\\u00B1\" + str(np.std(ll)))\n",
    "print('KL (test): ' + str(np.mean(kl)) + u\"\\u00B1\" + str(np.std(kl)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2135beb0",
   "metadata": {},
   "source": [
    "Next, we multiply a Periodic and RBF covariance function to obtain a Locally Periodic covariance function:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0c9ce6c",
   "metadata": {},
   "outputs": [],
   "source": [
    " # Model settings:\n",
    "gpkernel = jk.RBF() * jk.Periodic()\n",
    "num_particles = 1000\n",
    "num_mcmc_steps = 3000\n",
    "_, num_trials, d = Y.shape\n",
    "\n",
    "mse_cov_train, mse_samples_cov_train, mse_cov_pred, mse_samples_cov_pred, ll, kl = [], [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data and testing data:\n",
    "    x_train, Y_train = np.tile(x[:300], (1, d)), Y[:300, trial]\n",
    "    x_test, Y_test = np.tile(x[300:], (1, d)), Y[300:, trial]\n",
    "    n_train, n_test = Y_train.shape[0], Y_test.shape[0]\n",
    "\n",
    "    # Set priors and initialize model:\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    priors = dict(kernel = [dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                            dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()),\n",
    "                                 period=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp()))],\n",
    "                likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "    model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "    \n",
    "    # Inference with SMC:\n",
    "    start = time.time()\n",
    "    key = jrnd.PRNGKey(10)\n",
    "    particles, num_iter, lml = model.inference(key, mode='gibbs-in-smc', \n",
    "                                                sampling_parameters=dict(num_particles=num_particles, \n",
    "                                                                        num_mcmc_steps=num_mcmc_steps))\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute posterior distribution:\n",
    "    key = jrnd.PRNGKey(5)\n",
    "    Sigma_train = jax.vmap(construct_wishart_Lvec)(particles.particles['f'], particles.particles['likelihood']['L_vec'])\n",
    "    Sigma_pred = model.predict_Sigma(key, x_test)\n",
    "\n",
    "    # Evaluate performance per trial:\n",
    "    true_Sigma_train_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[:300])).flatten()\n",
    "    Sigma_train_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_train), (num_particles, -1))\n",
    "    mse_cov_train.append(mse(true_Sigma_train_triu, np.mean(Sigma_train_triu, axis=0)))\n",
    "    mse_samples_cov_train.append(mse(true_Sigma_train_triu, Sigma_train_triu))\n",
    "    true_Sigma_pred_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[300:])).flatten()\n",
    "    Sigma_pred_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_pred), (num_particles, -1))\n",
    "    mse_cov_pred.append(mse(true_Sigma_pred_triu, np.mean(Sigma_pred_triu, axis=0)))\n",
    "    mse_samples_cov_pred.append(mse(true_Sigma_pred_triu, Sigma_pred_triu))\n",
    "    ll.append(log_likelihood(Y_test, np.zeros((n_test, d, d)), Sigma_pred))\n",
    "\n",
    "    # Obtain the predictive posterior distribution and compute the KL-divergence:\n",
    "    keys = jrnd.split(key, n_test)\n",
    "    Y_pred = jax.vmap(lambda Sigma_sample: jax.vmap(lambda key, mu, cov: jrnd.multivariate_normal(key, mu, cov))(keys, jnp.zeros((n_test, d)), Sigma_sample))(Sigma_pred)\n",
    "    keys = jrnd.split(keys[-1], n_test)\n",
    "    k = jax.vmap(kl_divergence, in_axes=(0, 0, 0, 1))(keys, np.zeros((n_test, d)), true_Sigma[300:], Y_pred)\n",
    "    kl.append(np.mean(k))\n",
    "\n",
    "# Print performance:\n",
    "print('MSE (train): ' + str(np.mean(mse_cov_train)) + u\"\\u00B1\" + str(np.std(mse_cov_train)))\n",
    "print('MSE samples (train): ' + str(np.mean(mse_samples_cov_train)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_train)))\n",
    "print('MSE (test): ' + str(np.mean(mse_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_cov_pred)))\n",
    "print('MSE samples (test): ' + str(np.mean(mse_samples_cov_pred)) + u\"\\u00B1\" + str(np.std(mse_samples_cov_pred)))\n",
    "print('LL (test): ' + str(np.mean(ll)) + u\"\\u00B1\" + str(np.std(ll)))\n",
    "print('KL (test): ' + str(np.mean(kl)) + u\"\\u00B1\" + str(np.std(kl)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
