{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e9fc27",
   "metadata": {},
   "source": [
    "# Simulation study: learning the model parameters\n",
    "\n",
    "In this notebook, we illustrate the first simulation study of our work (https://arxiv.org/abs/2406.04796). In this simulation study, we construct an covariance process between $d=3$ variables from the prior of a Wishart process, and sample $n=300$ observations from a multivariate normal distribution with a mean of zero and the constructed covariance process. We generate 10 different datasets with the same underlying covariance process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42274885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "SELECTED_DEVICE = None\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = f''\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "import jax\n",
    "import sys\n",
    "jax.config.update(\"jax_default_device\", jax.devices()[0])\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrnd\n",
    "import distrax as dx\n",
    "import jaxkern as jk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "#import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "module_path = os.path.abspath(os.path.join('../bayesianmodels/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from uicsmodels.gaussianprocesses.fullwp import FullLatentWishartModel\n",
    "from uicsmodels.gaussianprocesses.wputil import vec2tril, tril2vec, construct_wishart, construct_wishart_Lvec\n",
    "from uicsmodels.gaussianprocesses.likelihoods import AbstractLikelihood\n",
    "from uicsmodels.gaussianprocesses.gputil import sample_predictive\n",
    "import time\n",
    "module_path = os.path.abspath(os.path.join('../BANNER/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import gpflow\n",
    "from gpflow.kernels import SharedIndependent, SquaredExponential, Matern12\n",
    "from gpflow import Parameter\n",
    "from gpflow.inducing_variables import InducingPoints, SharedIndependentInducingVariables\n",
    "import tensorflow as tf\n",
    "from src.likelihoods.WishartProcessLikelihood import WishartLikelihood\n",
    "from src.models.WishartProcess import WishartProcess\n",
    "from util.training_util import run_adam\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Data settings:\n",
    "gpkernel = jk.RBF()\n",
    "n = 300\n",
    "d = 3\n",
    "\n",
    "# Generate data:\n",
    "x = np.reshape(np.linspace(0, 1, n), (-1, 1))\n",
    "Y = np.zeros((n, 10, d))\n",
    "len_l = int(d * (d+1) / 2)\n",
    "priors = dict(kernel=dict(lengthscale=dx.Uniform(0.34999, 0.35001)), # set RBF lengthscale to 0.35.\n",
    "                likelihood=dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "model = FullLatentWishartModel(x, Y[:, 0, :], cov_fn=gpkernel, priors=priors)\n",
    "particles = model.init_fn(jrnd.PRNGKey(10), 10)\n",
    "true_Sigma = jax.vmap(construct_wishart_Lvec, in_axes=(0, None))(particles.position['f'], jnp.array([1., 0., 1., 0., 0., 1.]))\n",
    "for rep in range(10):\n",
    "    for i in range(n):\n",
    "        Y[i, rep, :] = np.random.multivariate_normal(mean=np.zeros((d,)), cov=np.array(true_Sigma[rep, i, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8bccd66d",
   "metadata": {},
   "source": [
    "### Performance metrics\n",
    "We compare MCMC, VI and SMC based on how well they recover the ground truth by the mean squared error (MSE) between the ground truth covariance process and the corresponding mean estimate of the covariance process (averaged over $d$ variables and $n$ observations). Additionally, we compute the MSE between the true and estimated lengthscale (0.35) and scale matrix (identity matrix). Finally, to evaluate the full posterior $\\textit{distribution}$, we compute the MSE for all individual samples of the covariance process, referred to as $\\text{MSE}_\\text{samples}$. This $\\text{MSE}_\\text{samples}$ is again averaged over all $d$ variables and $n$ observations."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dd21263f",
   "metadata": {},
   "outputs": [],
   "source": [
    "def mse(ground_truth, estimate):\n",
    "    \"\"\" Function to compute the mean squared error between a ground truth and a corresponding estimate.\n",
    "\n",
    "    Args:\n",
    "        ground_truth: a matrix or vector with the true values of the data.\n",
    "        estimate: a matrix or vector with the estimate(s) of the ground truth by the model. If 'estimate' is of\n",
    "                  exactly the same shape as 'ground_truth', the MSE is averaged over all dimensions (variables \n",
    "                  and observations). Alternatively, if 'estimate' consists of individual samples, the MSE is\n",
    "                  computed for each sample individually.\n",
    "\n",
    "    Returns: a scalar with the MSE or MSE_samples.\n",
    "    \"\"\"\n",
    "    if ground_truth.shape == estimate.shape: # MSE\n",
    "        return np.mean(np.square(ground_truth - estimate))\n",
    "    elif len(ground_truth.shape) < len(estimate.shape): # MSE_samples\n",
    "        mses = jax.vmap(lambda gt, est: jnp.mean(jnp.square(gt - est)), in_axes=(None, 0))(ground_truth, estimate)\n",
    "        return np.mean(mses)\n",
    "\n",
    "\n",
    "def L_to_V(L_sample, d):\n",
    "    \"\"\" This function computes the scale matrix from its lower Cholesky decomposition, and takes the upper triangular \n",
    "    elements of this matrix.\n",
    "\n",
    "    Args:\n",
    "        L_sample: a vector with the lower Cholesky decomposition of a scale matrix.\n",
    "        d: a scalar representing the number of variables.\n",
    "    \n",
    "    Returns: a vector with the upper triangular elements of the scale matrix corresponding to L_sample.\n",
    "    \"\"\" \n",
    "    L = jnp.zeros((d, d))\n",
    "    L = L.at[jnp.tril_indices(d)].set(L_sample)\n",
    "    return jnp.dot(L, L.T)[jnp.triu_indices(d)]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c20467",
   "metadata": {},
   "source": [
    "### MCMC sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ff22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.RBF()\n",
    "num_burn = 4000000\n",
    "num_samples = 1000000\n",
    "num_thin = 1000\n",
    "num_chains = 4\n",
    "num_thin_samples = num_samples // num_thin\n",
    "\n",
    "n, num_trials, d = Y.shape\n",
    "mse_cov, mse_samples_cov, mse_lengthscale, mse_V, rhats = [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data:\n",
    "    x_train, Y_train = x, Y[:, trial, :]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    # We combine the samples across multiple chains:\n",
    "    Sigma_train = np.zeros((num_thin_samples, n, d, d)) # array to store samples across chains.\n",
    "    Sigmas_chains = np.zeros((num_thin_samples, num_chains, n * len_l))\n",
    "    for repetition in range(num_chains):\n",
    "        # Set priors and initialize model:\n",
    "        priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                      likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "        model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "\n",
    "        # Inference with SMC:\n",
    "        start = time.time()\n",
    "        key = jrnd.PRNGKey(repetition)\n",
    "        states = model.inference(key, mode='gibbs', sampling_parameters=dict(num_burn=num_burn,\n",
    "                                                                            num_samples=num_samples,\n",
    "                                                                            num_thin=num_thin))\n",
    "        end = time.time()\n",
    "\n",
    "        # Compute posterior distribution:\n",
    "        Sigma_chain = jax.vmap(construct_wishart_Lvec)(model.states.position['f'], model.states.position['likelihood']['L_vec'])\n",
    "        Sigmas_chains[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_thin_samples, -1))\n",
    "        indices = np.random.choice(np.arange(num_thin_samples), size=(num_thin_samples // num_chains,), replace=False)\n",
    "        Sigma_train[repetition * (num_thin_samples // num_chains):(repetition+1) * (num_thin_samples // num_chains)] = Sigma_chain[indices, :, :, :]\n",
    "\n",
    "    # Compute convergence of covariance processes:\n",
    "    rhats.append(np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_chains)))\n",
    "\n",
    "    # Evaluate performance per trial:\n",
    "    true_Sigma_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[trial])).flatten()\n",
    "    est_Sigma_triu = np.reshape(jax.vmap(lambda sample_sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sample_sigma))(Sigma_train), (num_thin_samples, -1))\n",
    "    mse_cov.append(mse(true_Sigma_triu, np.mean(est_Sigma_triu, axis=0)))\n",
    "    mse_samples_cov.append(mse(true_Sigma_triu, est_Sigma_triu))\n",
    "    mse_lengthscale.append(mse(np.array(0.35), np.mean(model.states.position['kernel']['lengthscale'])))\n",
    "    V_distribution = jax.vmap(L_to_V, in_axes=(0, None))(model.states.position['likelihood']['L_vec'], d) # transform L to V.\n",
    "    mse_V.append(mse(np.eye(d)[np.triu_indices(d)], np.mean(V_distribution, axis=0)))\n",
    "\n",
    "# Print performance:\n",
    "print('PSRF: ' + str(np.mean(rhats)) + u\"\\u00B1\" + str(np.std(rhats)))\n",
    "print('MSE: ' + str(np.mean(mse_cov)) + u\"\\u00B1\" + str(np.std(mse_cov)))\n",
    "print('MSE samples: ' + str(np.mean(mse_samples_cov)) + u\"\\u00B1\" + str(np.std(mse_samples_cov)))\n",
    "print('MSE lengthscale: ' + str(np.mean(mse_lengthscale)) + u\"\\u00B1\" + str(np.std(mse_lengthscale)))\n",
    "print('MSE V: ' + str(np.mean(mse_V)) + u\"\\u00B1\" + str(np.std(mse_V)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5f41c",
   "metadata": {},
   "source": [
    "### Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5faa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "num_iterations = 1000000 # until convergence of the ELBO.\n",
    "num_samples = 1000\n",
    "num_initializations = 4\n",
    "n, num_trials, d = Y.shape\n",
    "nu = d + 1\n",
    "latent_dim = int(nu * d)\n",
    "\n",
    "# 10 fold cross validation:\n",
    "mse_cov, mse_samples_cov, mse_lengthscale, mse_V, rhats = [], [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data:\n",
    "    x_train, Y_train = np.tile(x, (1, d)), Y[:, trial, :]\n",
    "    \n",
    "    best_elbo = -np.inf \n",
    "    Sigmas_initializations = np.zeros((num_samples, num_initializations, n * (latent_dim // 2)))\n",
    "    \n",
    "    for repetition in range(num_initializations):\n",
    "        # Set GP kernel function, likelihood, and initialize model:\n",
    "        lengthscale_rbf = np.exp(np.random.normal())\n",
    "        gpkernel = SharedIndependent(SquaredExponential(lengthscales=lengthscale_rbf, variance=1.0), output_dim=latent_dim) \n",
    "        V = np.random.normal(size=int(d * nu // 2)) # initialize scale matrix.\n",
    "        V = Parameter(V)\n",
    "        inducing_points = tf.identity(x_train)\n",
    "        inducing_variable = SharedIndependentInducingVariables(InducingPoints(tf.identity(inducing_points)))\n",
    "        likelihood = WishartLikelihood(d, nu, A=V, N=n, R=3, additive_noise=True, model_inverse=False, model_mean=True)\n",
    "        model = WishartProcess(gpkernel, likelihood, D=d, nu=nu, inducing_variable=inducing_variable, num_data=n)\n",
    "        gpflow.set_trainable(model.inducing_variable, False) # we do not want to use inducing variables.\n",
    "\n",
    "        # Inference until convergence of the ELBO:\n",
    "        start = time.time()\n",
    "        logf = run_adam(model, (x_train, Y_train), num_iterations, minibatch_size=n, learning_rate=0.001)\n",
    "        end = time.time()\n",
    "\n",
    "        # Store estimates with the best ELBO:\n",
    "        Sigma_train_repetition = model.predict_mc(x_train, num_samples)\n",
    "        Sigmas_initializations[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(np.array(Sigma_train_repetition)), (num_samples, -1))\n",
    "        if logf[-1] > best_elbo:\n",
    "            best_elbo = logf[-1]\n",
    "            best_rep = repetition\n",
    "            Sigma_train = np.array(Sigma_train_repetition)\n",
    "    \n",
    "    # Compute convergence of covariance processes and evaluate performance per trial:\n",
    "    rhats.append(np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_initializations)))\n",
    "    true_Sigma_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[trial])).flatten()\n",
    "    mse_cov.append(mse(true_Sigma_triu, np.mean(Sigmas_initializations[:, best_rep, :], axis=0)))\n",
    "    mse_samples_cov.append(mse(true_Sigma_triu, Sigmas_initializations[:, best_rep, :]))\n",
    "    mse_lengthscale.append(mse(np.array(0.35), np.array(model.kernel.kernel.lengthscales)))\n",
    "    V_est = L_to_V(np.array(model.likelihood.A), d) # transform L to V.\n",
    "    mse_V.append(mse(np.eye(d)[np.triu_indices(d)], V_est))\n",
    "\n",
    "# Print performance:\n",
    "print('PSRF: ' + str(np.mean(rhats)) + u\"\\u00B1\" + str(np.std(rhats)))\n",
    "print('MSE: ' + str(np.mean(mse_cov)) + u\"\\u00B1\" + str(np.std(mse_cov)))\n",
    "print('MSE samples: ' + str(np.mean(mse_samples_cov)) + u\"\\u00B1\" + str(np.std(mse_samples_cov)))\n",
    "print('MSE lengthscale: ' + str(np.mean(mse_lengthscale)) + u\"\\u00B1\" + str(np.std(mse_lengthscale)))\n",
    "print('MSE V: ' + str(np.mean(mse_V)) + u\"\\u00B1\" + str(np.std(mse_V)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80cb6f",
   "metadata": {},
   "source": [
    "### Sequential Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52614cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.RBF()\n",
    "num_particles = 1000\n",
    "num_mcmc_steps = 2000\n",
    "\n",
    "n_train, num_trials, d = Y.shape\n",
    "mse_cov, mse_samples_cov, mse_lengthscale, mse_V = [], [], [], [] # lists to store results.\n",
    "for trial in range(num_trials):\n",
    "    # Get training data:\n",
    "    x_train, Y_train = x, Y[:, trial, :]\n",
    "    \n",
    "    # Set priors and initialize model:\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                  likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "    model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "    \n",
    "    # Inference with SMC:\n",
    "    start = time.time()\n",
    "    key = jrnd.PRNGKey(10)\n",
    "    particles, num_iter, lml = model.inference(key, mode='gibbs-in-smc', \n",
    "                                                sampling_parameters=dict(num_particles=num_particles, \n",
    "                                                                        num_mcmc_steps=num_mcmc_steps))\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute posterior distribution:\n",
    "    key = jrnd.PRNGKey(5)\n",
    "    Sigma_train = jax.vmap(construct_wishart_Lvec)(particles.particles['f'], particles.particles['likelihood']['L_vec'])\n",
    "    \n",
    "    # Evaluate performance per trial:\n",
    "    Sigma_train = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_train), (num_particles, -1))\n",
    "    true_Sigma_triu = (jax.vmap(lambda s: s[np.triu_indices(d)])(true_Sigma[trial])).flatten()\n",
    "    mse_cov.append(mse(true_Sigma_triu, np.mean(Sigma_train, axis=0)))\n",
    "    mse_samples_cov.append(mse(true_Sigma_triu, Sigma_train))\n",
    "    mse_lengthscale.append(mse(np.array(0.35), np.mean(particles.particles['kernel']['lengthscale'])))\n",
    "    V_distribution = jax.vmap(L_to_V, in_axes=(0, None))(particles.particles['likelihood']['L_vec'], d) # transform L to V.\n",
    "    mse_V.append(mse(np.eye(d)[np.triu_indices(d)], np.mean(V_distribution, axis=0)))\n",
    "\n",
    "# Print performance:\n",
    "print('MSE: ' + str(np.mean(mse_cov)) + u\"\\u00B1\" + str(np.std(mse_cov)))\n",
    "print('MSE samples: ' + str(np.mean(mse_samples_cov)) + u\"\\u00B1\" + str(np.std(mse_samples_cov)))\n",
    "print('MSE lengthscale: ' + str(np.mean(mse_lengthscale)) + u\"\\u00B1\" + str(np.std(mse_lengthscale)))\n",
    "print('MSE V: ' + str(np.mean(mse_V)) + u\"\\u00B1\" + str(np.std(mse_V)))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
