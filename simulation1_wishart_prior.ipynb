{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d1e9fc27",
   "metadata": {},
   "source": [
    "# Simulation study: learning the model parameters\n",
    "\n",
    "In this notebook, we illustrate the first simulation study of our work (https://arxiv.org/abs/2406.04796). In this simulation study, we construct an covariance process between $d=3$ variables from the prior of a Wishart process, and sample $n=300$ observations from a multivariate normal distribution with a mean of zero and the constructed covariance process. We generate 10 different datasets with the same underlying covariance process. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "42274885",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Imports\n",
    "import os\n",
    "SELECTED_DEVICE = None\n",
    "os.environ['CUDA_VISIBLE_DEVICES'] = f''\n",
    "os.environ['JAX_PLATFORM_NAME'] = 'cpu'\n",
    "import jax\n",
    "import sys\n",
    "jax.config.update(\"jax_default_device\", jax.devices()[0])\n",
    "jax.config.update(\"jax_enable_x64\", True)\n",
    "import jax.numpy as jnp\n",
    "import jax.random as jrnd\n",
    "import distrax as dx\n",
    "import jaxkern as jk\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from factor_analyzer import FactorAnalyzer\n",
    "from sklearn.preprocessing import PolynomialFeatures\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from tensorflow_probability.substrates import jax as tfp\n",
    "#import tensorflow_probability as tfp\n",
    "tfb = tfp.bijectors\n",
    "module_path = os.path.abspath(os.path.join('../bayesianmodels/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from uicsmodels.gaussianprocesses.fullwp import FullLatentWishartModel\n",
    "from uicsmodels.gaussianprocesses.wputil import vec2tril, tril2vec, construct_wishart, construct_wishart_Lvec\n",
    "from uicsmodels.gaussianprocesses.likelihoods import AbstractLikelihood\n",
    "from uicsmodels.gaussianprocesses.gputil import sample_predictive\n",
    "import time\n",
    "module_path = os.path.abspath(os.path.join('../BANNER/'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "import gpflow\n",
    "from gpflow.kernels import SharedIndependent, SquaredExponential, Matern12\n",
    "from gpflow import Parameter\n",
    "from gpflow.inducing_variables import InducingPoints, SharedIndependentInducingVariables\n",
    "import tensorflow as tf\n",
    "from src.likelihoods.WishartProcessLikelihood import WishartLikelihood\n",
    "from src.models.WishartProcess import WishartProcess\n",
    "from util.training_util import run_adam\n",
    "from scipy.stats import multivariate_normal\n",
    "\n",
    "# Data/model settings:\n",
    "gpkernel = jk.RBF()\n",
    "n = 300\n",
    "d = 3\n",
    "\n",
    "# Generate data:\n",
    "x = np.reshape(np.linspace(0, 1, n), (-1, 1))\n",
    "Y = np.zeros((n, 10, d))\n",
    "len_l = int(d * (d+1) / 2)\n",
    "priors = dict(kernel=dict(lengthscale=dx.Uniform(0.34999, 0.35001)), # set RBF lengthscale to 0.35.\n",
    "                likelihood=dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "model = FullLatentWishartModel(x, Y[:, 0, :], cov_fn=gpkernel, priors=priors)\n",
    "particles = model.init_fn(jrnd.PRNGKey(10), 10)\n",
    "true_Sigma = jax.vmap(construct_wishart_Lvec, in_axes=(0, None))(particles.position['f'], jnp.array([1., 0., 1., 0., 0., 1.]))\n",
    "for rep in range(10):\n",
    "    for i in range(n):\n",
    "        Y[i, rep, :] = np.random.multivariate_normal(mean=np.zeros((d,)), cov=np.array(true_Sigma[rep, i, :, :]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "68c20467",
   "metadata": {},
   "source": [
    "### MCMC sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "060ff22b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.RBF()\n",
    "num_burn = 4000000\n",
    "num_samples = 1000000\n",
    "num_thin = 1000\n",
    "num_chains = 4\n",
    "num_thin_samples = num_samples // num_thin\n",
    "\n",
    "n, num_trials, d = Y.shape\n",
    "for trial in range(num_trials):\n",
    "    # Get training data:\n",
    "    x_train, Y_train = x, Y[:, trial, :]\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    \n",
    "    # We combine the samples across multiple chains:\n",
    "    Sigma_train = np.zeros((num_thin_samples, n, d, d)) # array to store samples across chains.\n",
    "    Sigmas_chains = np.zeros((num_thin_samples, num_chains, n * len_l))\n",
    "    for repetition in range(num_chains):\n",
    "        # Set priors and initialize model:\n",
    "        priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                      likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "        model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "\n",
    "        # Inference with SMC:\n",
    "        start = time.time()\n",
    "        key = jrnd.PRNGKey(repetition)\n",
    "        states = model.inference(key, mode='gibbs', sampling_parameters=dict(num_burn=num_burn,\n",
    "                                                                            num_samples=num_samples,\n",
    "                                                                            num_thin=num_thin))\n",
    "        end = time.time()\n",
    "\n",
    "        # Compute posterior distribution:\n",
    "        Sigma_chain = jax.vmap(construct_wishart_Lvec)(model.states.position['f'], model.states.position['likelihood']['L_vec'])\n",
    "        Sigmas_chains[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(Sigma_chain), (num_thin_samples, -1))\n",
    "        indices = np.random.choice(np.arange(num_thin_samples), size=(num_thin_samples // num_chains,), replace=False)\n",
    "        Sigma_train[repetition * (num_thin_samples // num_chains):(repetition+1) * (num_thin_samples // num_chains)] = Sigma_chain[indices, :, :, :]\n",
    "\n",
    "    # Compute convergence of covariance processes:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_chains))\n",
    "    print(rhat)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37e5f41c",
   "metadata": {},
   "source": [
    "### Variational inference"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7a5faa84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "num_iterations = 1000000 # until convergence of the ELBO.\n",
    "num_samples = 1000\n",
    "num_initializations = 4\n",
    "n, num_trials, d = Y.shape\n",
    "nu = d + 1\n",
    "latent_dim = int(nu * d)\n",
    "\n",
    "# 10 fold cross validation:\n",
    "for trial in range(num_trials):\n",
    "    # Get training data:\n",
    "    x_train, Y_train = np.tile(x, (1, d)), Y[:, trial, :]\n",
    "    \n",
    "    best_elbo = -np.inf \n",
    "    Sigmas_initializations = np.zeros((num_samples, num_initializations, n * (latent_dim // 2)))\n",
    "    \n",
    "    for repetition in range(num_initializations):\n",
    "        # Set GP kernel function, likelihood, and initialize model:\n",
    "        lengthscale_rbf = np.exp(np.random.normal())\n",
    "        gpkernel = SharedIndependent(SquaredExponential(lengthscales=lengthscale_rbf, variance=1.0), output_dim=latent_dim) \n",
    "        V = np.random.normal(size=int(d * nu // 2)) # initialize scale matrix.\n",
    "        V = Parameter(V)\n",
    "        inducing_points = tf.identity(x_train)\n",
    "        inducing_variable = SharedIndependentInducingVariables(InducingPoints(tf.identity(inducing_points)))\n",
    "        likelihood = WishartLikelihood(d, nu, A=V, N=n, R=3, additive_noise=True, model_inverse=False, model_mean=True)\n",
    "        model = WishartProcess(gpkernel, likelihood, D=d, nu=nu, inducing_variable=inducing_variable, num_data=n)\n",
    "        gpflow.set_trainable(model.inducing_variable, False) # we do not want to use inducing variables.\n",
    "\n",
    "        # Inference until convergence of the ELBO:\n",
    "        start = time.time()\n",
    "        logf = run_adam(model, (x_train, Y_train), num_iterations, minibatch_size=n, learning_rate=0.001)\n",
    "        end = time.time()\n",
    "\n",
    "        # Store estimates with the best ELBO:\n",
    "        Sigma_train_repetition = model.predict_mc(x_train, num_samples)\n",
    "        Sigmas_initializations[:, repetition, :] = np.reshape(jax.vmap(lambda sigma: jax.vmap(lambda s: s[np.triu_indices(d)])(sigma))(np.array(Sigma_train_repetition)), (num_samples, -1))\n",
    "        if logf[-1] > best_elbo:\n",
    "            best_elbo = logf[-1]\n",
    "            Sigma_train = np.array(Sigma_train_repetition)\n",
    "    \n",
    "    # Compute convergence of covariance processes:\n",
    "    rhat = np.mean(tfp.mcmc.diagnostic.potential_scale_reduction(Sigmas_initializations))\n",
    "    print(rhat)\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e80cb6f",
   "metadata": {},
   "source": [
    "### Sequential Monte Carlo sampling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52614cbf",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model settings:\n",
    "gpkernel = jk.RBF()\n",
    "num_particles = 1000\n",
    "num_mcmc_steps = 2000\n",
    "\n",
    "n_train, num_trials, d = Y.shape\n",
    "for trial in range(num_trials):\n",
    "    # Get training data:\n",
    "    x_train, Y_train = x, Y[:, trial, :]\n",
    "    \n",
    "    # Set priors and initialize model:\n",
    "    len_l = int(d * (d+1) / 2)\n",
    "    priors = dict(kernel = dict(lengthscale=dx.Transformed(dx.Normal(loc=0., scale=1.), tfb.Exp())),\n",
    "                  likelihood = dict(L_vec=dx.Normal(loc=jnp.zeros((len_l, )), scale=jnp.ones((len_l, )))))\n",
    "    model = FullLatentWishartModel(x_train, Y_train, cov_fn=gpkernel, priors=priors)\n",
    "    \n",
    "    # Inference with SMC:\n",
    "    start = time.time()\n",
    "    key = jrnd.PRNGKey(10)\n",
    "    particles, num_iter, lml = model.inference(key, mode='gibbs-in-smc', \n",
    "                                                sampling_parameters=dict(num_particles=num_particles, \n",
    "                                                                        num_mcmc_steps=num_mcmc_steps))\n",
    "    end = time.time()\n",
    "\n",
    "    # Compute posterior distribution:\n",
    "    key = jrnd.PRNGKey(5)\n",
    "    Sigma_train = jax.vmap(construct_wishart_Lvec)(particles.particles['f'], particles.particles['likelihood']['L_vec'])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
